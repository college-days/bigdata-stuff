please checkout these pages if you want add any repos about spark in the pom file

* [org.apache.spark / spark-core_2.10 / 1.0.0-cdh5.1.3](http://maven-repository.com/artifact/org.apache.spark/spark-core_2.10/1.0.0-cdh5.1.3)
* [org.apache.spark / spark-sql_2.10 / 1.0.0-cdh5.1.3](http://maven-repository.com/artifact/org.apache.spark/spark-sql_2.10/1.0.0-cdh5.1.3) 
* [org.apache.spark](http://maven-repository.com/artifact/org.apache.spark) 

## how to start

import the maven project to idea

## get jar file

```
mvn package
```

## how to run

* common use

```
scp target/news-spark-1.0-SNAPSHOT.jar root@192.168.1.103:~/
spark-submit --class org.give.newsspark.WordCount --master yarn news-spark-1.0-SNAPSHOT.jar /user/root/cleantha.txt
```
`org.give.newsspark.WordCount`and`/user/root/cleantha.txt`is just the main class name and the file path in hdfs, so it's up to you

* or you can just run the spark app in this way

```
spark-submit --class org.give.newsspark.FilterTrainPositive --master yarn-cluster news-spark-1.0-SNAPSHOT.jar
```

you can use `yarn-cluster`, but if you want to checkout the logs you should view the ui view showed in `appTrackingUrl`, for example `http://work01:8088/proxy/application_1412475343976_0003/A`, to watch the log generated by yarn

* so i recommend to use yarn instead of yarn-cluster

## 原始数据的坑

因为原始数据存在同一个用户id在同一个时间戳上会点击多个新闻的坑，导致一开始产生的训练正例有10078个样本，所以需要使用[FilterTrainPositive](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/newsspark/FilterTrainPositive.scala)来消除这种duplicated的情况

## cleantha

>In some dataset, they add the ratings to the training set. A better way to get the recommendations is training a matrix factorization model first and then augmenting the model using some ratings. If this sounds interesting, take a look at the implementation of MatrixFactorizationModel
是矩阵分解和als结合的意思

* [MatrixFactorizationModel](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala)
* [1.0的实现更简单，就一个predict](https://github.com/apache/spark/blob/branch-1.0/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala)

>是矩阵分解和als结合的意思，反正两个都跑通了

* [singular decomposition](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/playspark/SingularDecomp.scala)
* [collaborate filtering](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/playspark/CFonStandalone.scala)

## cf

[explicitfeedback](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/newsspark/cfyarn/ExplicitWithBestModel.scala)和[implicitfeedback](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/newsspark/cfyarn/ImplicitWithBestModel.scala)如果把推荐数目一增大，那么使用spark-submit执行job就会出现很奇怪的DAGSchduler的异常，然后一定要在spark-shell中先[小范围](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/newsspark/cfyarn/ExplicitWithBestModel.scala#L54)地跑一次，然后再对[所有样本](https://github.com/zjhlocl/news-spark/blob/master/news-spark/src/main/scala/org/give/newsspark/cfyarn/ExplicitWithBestModel.scala#L53)进行推荐，都一定要在spark-shell中进行，很奇怪的bug
